{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label   I1        I2    I3    I4        I5     I6    I7    I8     I9  ...  \\\n",
      "0      1  0.0  0.008292  0.11  0.10  0.160344  0.068  0.02  0.08  0.010  ...   \n",
      "1      0  0.0  0.003317  0.00  0.00  0.077438  0.000  0.00  0.74  0.194  ...   \n",
      "2      1  1.0  0.533997  0.00  0.08  0.000078  0.008  0.89  0.80  0.176  ...   \n",
      "3      0  0.0  0.097844  0.02  0.00  0.000000  0.000  0.00  0.00  0.002  ...   \n",
      "4      0  0.0  0.003317  0.00  0.00  1.000000  0.284  0.00  0.14  0.126  ...   \n",
      "\n",
      "       C17      C18      C19      C20      C21      C22      C23      C24  \\\n",
      "0  1528983  1528994  1534050  1536021  1536022  1934144  1934163  1934311   \n",
      "1  1528986  1529049  1533924  1536018  1536023  1934144  1934169  1934181   \n",
      "2  1528988  1529017  1533924  1536018  1536209  1934144  1934163  1934312   \n",
      "3  1528990  1529030  1533924  1536018  1536025  1934144  1934167  1934181   \n",
      "4  1528983  1529049  1533924  1536018  1536023  1934144  1934164  1934181   \n",
      "\n",
      "       C25      C26  \n",
      "0  2022806  2024736  \n",
      "1  2022801  2022897  \n",
      "2  2022801  2022897  \n",
      "3  2022801  2022897  \n",
      "4  2022801  2022897  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "(100000, 27)\n",
      "Index(['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
      "       'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21',\n",
      "       'C22', 'C23', 'C24', 'C25', 'C26', 'label'],\n",
      "      dtype='object')\n",
      "{'label': 2, 'C9': 3, 'C20': 4, 'C17': 10, 'C22': 11, 'C6': 12, 'C23': 14, 'C14': 26, 'C25': 52, 'C5': 127, 'C8': 237, 'C1': 465, 'C2': 491, 'C19': 1072, 'C18': 2169, 'C13': 2681, 'C11': 3605, 'C15': 4516, 'C26': 5218, 'C24': 6650, 'C7': 7267, 'C10': 9282, 'C4': 11255, 'C16': 11486, 'C21': 11655, 'C3': 11751, 'C12': 11796}\n",
      "C1          1400\n",
      "C2          2025\n",
      "C3        415194\n",
      "C4        663886\n",
      "C5        664500\n",
      "C6        664532\n",
      "C7        676689\n",
      "C8        677339\n",
      "C9        677369\n",
      "C10       731077\n",
      "C11       737368\n",
      "C12      1146810\n",
      "C13      1150506\n",
      "C14      1150537\n",
      "C15      1162930\n",
      "C16      1527647\n",
      "C17      1528991\n",
      "C18      1533741\n",
      "C19      1535787\n",
      "C20      1536021\n",
      "C21      1933645\n",
      "C22      1934156\n",
      "C23      1934176\n",
      "C24      2022373\n",
      "C25      2022865\n",
      "C26      2086688\n",
      "label          1\n",
      "dtype: int64\n",
      "C1 has 465 classes\n",
      "C2 has 491 classes\n",
      "C3 has 11751 classes\n",
      "C4 has 11255 classes\n",
      "C5 has 127 classes\n",
      "C6 has 12 classes\n",
      "C7 has 7267 classes\n",
      "C8 has 237 classes\n",
      "C9 has 3 classes\n",
      "C10 has 9282 classes\n",
      "C11 has 3605 classes\n",
      "C12 has 11796 classes\n",
      "C13 has 2681 classes\n",
      "C14 has 26 classes\n",
      "C15 has 4516 classes\n",
      "C16 has 11486 classes\n",
      "C17 has 10 classes\n",
      "C18 has 2169 classes\n",
      "C19 has 1072 classes\n",
      "C20 has 4 classes\n",
      "C21 has 11655 classes\n",
      "C22 has 11 classes\n",
      "C23 has 14 classes\n",
      "C24 has 6650 classes\n",
      "C25 has 52 classes\n",
      "C26 has 5218 classes\n",
      "label has 2 classes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ModelsMap:\n\tsize mismatch for models.C1.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 906]) from checkpoint, the shape in current model is torch.Size([400, 465]).\n\tsize mismatch for models.C1.Decoder.FC_output.weight: copying a param with shape torch.Size([906, 400]) from checkpoint, the shape in current model is torch.Size([465, 400]).\n\tsize mismatch for models.C1.Decoder.FC_output.bias: copying a param with shape torch.Size([906]) from checkpoint, the shape in current model is torch.Size([465]).\n\tsize mismatch for models.C2.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 524]) from checkpoint, the shape in current model is torch.Size([400, 491]).\n\tsize mismatch for models.C2.Decoder.FC_output.weight: copying a param with shape torch.Size([524, 400]) from checkpoint, the shape in current model is torch.Size([491, 400]).\n\tsize mismatch for models.C2.Decoder.FC_output.bias: copying a param with shape torch.Size([524]) from checkpoint, the shape in current model is torch.Size([491]).\n\tsize mismatch for models.C3.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 43361]) from checkpoint, the shape in current model is torch.Size([400, 11751]).\n\tsize mismatch for models.C3.Decoder.FC_output.weight: copying a param with shape torch.Size([43361, 400]) from checkpoint, the shape in current model is torch.Size([11751, 400]).\n\tsize mismatch for models.C3.Decoder.FC_output.bias: copying a param with shape torch.Size([43361]) from checkpoint, the shape in current model is torch.Size([11751]).\n\tsize mismatch for models.C4.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 40131]) from checkpoint, the shape in current model is torch.Size([400, 11255]).\n\tsize mismatch for models.C4.Decoder.FC_output.weight: copying a param with shape torch.Size([40131, 400]) from checkpoint, the shape in current model is torch.Size([11255, 400]).\n\tsize mismatch for models.C4.Decoder.FC_output.bias: copying a param with shape torch.Size([40131]) from checkpoint, the shape in current model is torch.Size([11255]).\n\tsize mismatch for models.C5.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 211]) from checkpoint, the shape in current model is torch.Size([400, 127]).\n\tsize mismatch for models.C5.Decoder.FC_output.weight: copying a param with shape torch.Size([211, 400]) from checkpoint, the shape in current model is torch.Size([127, 400]).\n\tsize mismatch for models.C5.Decoder.FC_output.bias: copying a param with shape torch.Size([211]) from checkpoint, the shape in current model is torch.Size([127]).\n\tsize mismatch for models.C6.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 14]) from checkpoint, the shape in current model is torch.Size([400, 12]).\n\tsize mismatch for models.C6.Decoder.FC_output.weight: copying a param with shape torch.Size([14, 400]) from checkpoint, the shape in current model is torch.Size([12, 400]).\n\tsize mismatch for models.C6.Decoder.FC_output.bias: copying a param with shape torch.Size([14]) from checkpoint, the shape in current model is torch.Size([12]).\n\tsize mismatch for models.C7.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 9601]) from checkpoint, the shape in current model is torch.Size([400, 7267]).\n\tsize mismatch for models.C7.Decoder.FC_output.weight: copying a param with shape torch.Size([9601, 400]) from checkpoint, the shape in current model is torch.Size([7267, 400]).\n\tsize mismatch for models.C7.Decoder.FC_output.bias: copying a param with shape torch.Size([9601]) from checkpoint, the shape in current model is torch.Size([7267]).\n\tsize mismatch for models.C8.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 426]) from checkpoint, the shape in current model is torch.Size([400, 237]).\n\tsize mismatch for models.C8.Decoder.FC_output.weight: copying a param with shape torch.Size([426, 400]) from checkpoint, the shape in current model is torch.Size([237, 400]).\n\tsize mismatch for models.C8.Decoder.FC_output.bias: copying a param with shape torch.Size([426]) from checkpoint, the shape in current model is torch.Size([237]).\n\tsize mismatch for models.C10.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 19111]) from checkpoint, the shape in current model is torch.Size([400, 9282]).\n\tsize mismatch for models.C10.Decoder.FC_output.weight: copying a param with shape torch.Size([19111, 400]) from checkpoint, the shape in current model is torch.Size([9282, 400]).\n\tsize mismatch for models.C10.Decoder.FC_output.bias: copying a param with shape torch.Size([19111]) from checkpoint, the shape in current model is torch.Size([9282]).\n\tsize mismatch for models.C11.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 4341]) from checkpoint, the shape in current model is torch.Size([400, 3605]).\n\tsize mismatch for models.C11.Decoder.FC_output.weight: copying a param with shape torch.Size([4341, 400]) from checkpoint, the shape in current model is torch.Size([3605, 400]).\n\tsize mismatch for models.C11.Decoder.FC_output.bias: copying a param with shape torch.Size([4341]) from checkpoint, the shape in current model is torch.Size([3605]).\n\tsize mismatch for models.C12.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 43823]) from checkpoint, the shape in current model is torch.Size([400, 11796]).\n\tsize mismatch for models.C12.Decoder.FC_output.weight: copying a param with shape torch.Size([43823, 400]) from checkpoint, the shape in current model is torch.Size([11796, 400]).\n\tsize mismatch for models.C12.Decoder.FC_output.bias: copying a param with shape torch.Size([43823]) from checkpoint, the shape in current model is torch.Size([11796]).\n\tsize mismatch for models.C13.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 2995]) from checkpoint, the shape in current model is torch.Size([400, 2681]).\n\tsize mismatch for models.C13.Decoder.FC_output.weight: copying a param with shape torch.Size([2995, 400]) from checkpoint, the shape in current model is torch.Size([2681, 400]).\n\tsize mismatch for models.C13.Decoder.FC_output.bias: copying a param with shape torch.Size([2995]) from checkpoint, the shape in current model is torch.Size([2681]).\n\tsize mismatch for models.C15.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 7110]) from checkpoint, the shape in current model is torch.Size([400, 4516]).\n\tsize mismatch for models.C15.Decoder.FC_output.weight: copying a param with shape torch.Size([7110, 400]) from checkpoint, the shape in current model is torch.Size([4516, 400]).\n\tsize mismatch for models.C15.Decoder.FC_output.bias: copying a param with shape torch.Size([7110]) from checkpoint, the shape in current model is torch.Size([4516]).\n\tsize mismatch for models.C16.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 44666]) from checkpoint, the shape in current model is torch.Size([400, 11486]).\n\tsize mismatch for models.C16.Decoder.FC_output.weight: copying a param with shape torch.Size([44666, 400]) from checkpoint, the shape in current model is torch.Size([11486, 400]).\n\tsize mismatch for models.C16.Decoder.FC_output.bias: copying a param with shape torch.Size([44666]) from checkpoint, the shape in current model is torch.Size([11486]).\n\tsize mismatch for models.C18.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 3254]) from checkpoint, the shape in current model is torch.Size([400, 2169]).\n\tsize mismatch for models.C18.Decoder.FC_output.weight: copying a param with shape torch.Size([3254, 400]) from checkpoint, the shape in current model is torch.Size([2169, 400]).\n\tsize mismatch for models.C18.Decoder.FC_output.bias: copying a param with shape torch.Size([3254]) from checkpoint, the shape in current model is torch.Size([2169]).\n\tsize mismatch for models.C19.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 1623]) from checkpoint, the shape in current model is torch.Size([400, 1072]).\n\tsize mismatch for models.C19.Decoder.FC_output.weight: copying a param with shape torch.Size([1623, 400]) from checkpoint, the shape in current model is torch.Size([1072, 400]).\n\tsize mismatch for models.C19.Decoder.FC_output.bias: copying a param with shape torch.Size([1623]) from checkpoint, the shape in current model is torch.Size([1072]).\n\tsize mismatch for models.C21.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 44245]) from checkpoint, the shape in current model is torch.Size([400, 11655]).\n\tsize mismatch for models.C21.Decoder.FC_output.weight: copying a param with shape torch.Size([44245, 400]) from checkpoint, the shape in current model is torch.Size([11655, 400]).\n\tsize mismatch for models.C21.Decoder.FC_output.bias: copying a param with shape torch.Size([44245]) from checkpoint, the shape in current model is torch.Size([11655]).\n\tsize mismatch for models.C22.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 13]) from checkpoint, the shape in current model is torch.Size([400, 11]).\n\tsize mismatch for models.C22.Decoder.FC_output.weight: copying a param with shape torch.Size([13, 400]) from checkpoint, the shape in current model is torch.Size([11, 400]).\n\tsize mismatch for models.C22.Decoder.FC_output.bias: copying a param with shape torch.Size([13]) from checkpoint, the shape in current model is torch.Size([11]).\n\tsize mismatch for models.C23.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 15]) from checkpoint, the shape in current model is torch.Size([400, 14]).\n\tsize mismatch for models.C23.Decoder.FC_output.weight: copying a param with shape torch.Size([15, 400]) from checkpoint, the shape in current model is torch.Size([14, 400]).\n\tsize mismatch for models.C23.Decoder.FC_output.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([14]).\n\tsize mismatch for models.C24.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 21462]) from checkpoint, the shape in current model is torch.Size([400, 6650]).\n\tsize mismatch for models.C24.Decoder.FC_output.weight: copying a param with shape torch.Size([21462, 400]) from checkpoint, the shape in current model is torch.Size([6650, 400]).\n\tsize mismatch for models.C24.Decoder.FC_output.bias: copying a param with shape torch.Size([21462]) from checkpoint, the shape in current model is torch.Size([6650]).\n\tsize mismatch for models.C25.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 61]) from checkpoint, the shape in current model is torch.Size([400, 52]).\n\tsize mismatch for models.C25.Decoder.FC_output.weight: copying a param with shape torch.Size([61, 400]) from checkpoint, the shape in current model is torch.Size([52, 400]).\n\tsize mismatch for models.C25.Decoder.FC_output.bias: copying a param with shape torch.Size([61]) from checkpoint, the shape in current model is torch.Size([52]).\n\tsize mismatch for models.C26.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 16941]) from checkpoint, the shape in current model is torch.Size([400, 5218]).\n\tsize mismatch for models.C26.Decoder.FC_output.weight: copying a param with shape torch.Size([16941, 400]) from checkpoint, the shape in current model is torch.Size([5218, 400]).\n\tsize mismatch for models.C26.Decoder.FC_output.bias: copying a param with shape torch.Size([16941]) from checkpoint, the shape in current model is torch.Size([5218]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 183\u001b[0m\n\u001b[1;32m    179\u001b[0m columns_to_keep \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC7\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC9\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC11\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    180\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC12\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC13\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC14\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC15\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC16\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC17\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC18\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC19\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC20\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC21\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    181\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC22\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC23\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC24\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC25\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC26\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    182\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelsMap(columns_to_keep, num_classes, hidden_dim, \u001b[38;5;241m4\u001b[39m, DEVICE)         \n\u001b[0;32m--> 183\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnov_model_weights.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Load the criteox_1 dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlcore/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ModelsMap:\n\tsize mismatch for models.C1.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 906]) from checkpoint, the shape in current model is torch.Size([400, 465]).\n\tsize mismatch for models.C1.Decoder.FC_output.weight: copying a param with shape torch.Size([906, 400]) from checkpoint, the shape in current model is torch.Size([465, 400]).\n\tsize mismatch for models.C1.Decoder.FC_output.bias: copying a param with shape torch.Size([906]) from checkpoint, the shape in current model is torch.Size([465]).\n\tsize mismatch for models.C2.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 524]) from checkpoint, the shape in current model is torch.Size([400, 491]).\n\tsize mismatch for models.C2.Decoder.FC_output.weight: copying a param with shape torch.Size([524, 400]) from checkpoint, the shape in current model is torch.Size([491, 400]).\n\tsize mismatch for models.C2.Decoder.FC_output.bias: copying a param with shape torch.Size([524]) from checkpoint, the shape in current model is torch.Size([491]).\n\tsize mismatch for models.C3.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 43361]) from checkpoint, the shape in current model is torch.Size([400, 11751]).\n\tsize mismatch for models.C3.Decoder.FC_output.weight: copying a param with shape torch.Size([43361, 400]) from checkpoint, the shape in current model is torch.Size([11751, 400]).\n\tsize mismatch for models.C3.Decoder.FC_output.bias: copying a param with shape torch.Size([43361]) from checkpoint, the shape in current model is torch.Size([11751]).\n\tsize mismatch for models.C4.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 40131]) from checkpoint, the shape in current model is torch.Size([400, 11255]).\n\tsize mismatch for models.C4.Decoder.FC_output.weight: copying a param with shape torch.Size([40131, 400]) from checkpoint, the shape in current model is torch.Size([11255, 400]).\n\tsize mismatch for models.C4.Decoder.FC_output.bias: copying a param with shape torch.Size([40131]) from checkpoint, the shape in current model is torch.Size([11255]).\n\tsize mismatch for models.C5.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 211]) from checkpoint, the shape in current model is torch.Size([400, 127]).\n\tsize mismatch for models.C5.Decoder.FC_output.weight: copying a param with shape torch.Size([211, 400]) from checkpoint, the shape in current model is torch.Size([127, 400]).\n\tsize mismatch for models.C5.Decoder.FC_output.bias: copying a param with shape torch.Size([211]) from checkpoint, the shape in current model is torch.Size([127]).\n\tsize mismatch for models.C6.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 14]) from checkpoint, the shape in current model is torch.Size([400, 12]).\n\tsize mismatch for models.C6.Decoder.FC_output.weight: copying a param with shape torch.Size([14, 400]) from checkpoint, the shape in current model is torch.Size([12, 400]).\n\tsize mismatch for models.C6.Decoder.FC_output.bias: copying a param with shape torch.Size([14]) from checkpoint, the shape in current model is torch.Size([12]).\n\tsize mismatch for models.C7.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 9601]) from checkpoint, the shape in current model is torch.Size([400, 7267]).\n\tsize mismatch for models.C7.Decoder.FC_output.weight: copying a param with shape torch.Size([9601, 400]) from checkpoint, the shape in current model is torch.Size([7267, 400]).\n\tsize mismatch for models.C7.Decoder.FC_output.bias: copying a param with shape torch.Size([9601]) from checkpoint, the shape in current model is torch.Size([7267]).\n\tsize mismatch for models.C8.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 426]) from checkpoint, the shape in current model is torch.Size([400, 237]).\n\tsize mismatch for models.C8.Decoder.FC_output.weight: copying a param with shape torch.Size([426, 400]) from checkpoint, the shape in current model is torch.Size([237, 400]).\n\tsize mismatch for models.C8.Decoder.FC_output.bias: copying a param with shape torch.Size([426]) from checkpoint, the shape in current model is torch.Size([237]).\n\tsize mismatch for models.C10.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 19111]) from checkpoint, the shape in current model is torch.Size([400, 9282]).\n\tsize mismatch for models.C10.Decoder.FC_output.weight: copying a param with shape torch.Size([19111, 400]) from checkpoint, the shape in current model is torch.Size([9282, 400]).\n\tsize mismatch for models.C10.Decoder.FC_output.bias: copying a param with shape torch.Size([19111]) from checkpoint, the shape in current model is torch.Size([9282]).\n\tsize mismatch for models.C11.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 4341]) from checkpoint, the shape in current model is torch.Size([400, 3605]).\n\tsize mismatch for models.C11.Decoder.FC_output.weight: copying a param with shape torch.Size([4341, 400]) from checkpoint, the shape in current model is torch.Size([3605, 400]).\n\tsize mismatch for models.C11.Decoder.FC_output.bias: copying a param with shape torch.Size([4341]) from checkpoint, the shape in current model is torch.Size([3605]).\n\tsize mismatch for models.C12.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 43823]) from checkpoint, the shape in current model is torch.Size([400, 11796]).\n\tsize mismatch for models.C12.Decoder.FC_output.weight: copying a param with shape torch.Size([43823, 400]) from checkpoint, the shape in current model is torch.Size([11796, 400]).\n\tsize mismatch for models.C12.Decoder.FC_output.bias: copying a param with shape torch.Size([43823]) from checkpoint, the shape in current model is torch.Size([11796]).\n\tsize mismatch for models.C13.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 2995]) from checkpoint, the shape in current model is torch.Size([400, 2681]).\n\tsize mismatch for models.C13.Decoder.FC_output.weight: copying a param with shape torch.Size([2995, 400]) from checkpoint, the shape in current model is torch.Size([2681, 400]).\n\tsize mismatch for models.C13.Decoder.FC_output.bias: copying a param with shape torch.Size([2995]) from checkpoint, the shape in current model is torch.Size([2681]).\n\tsize mismatch for models.C15.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 7110]) from checkpoint, the shape in current model is torch.Size([400, 4516]).\n\tsize mismatch for models.C15.Decoder.FC_output.weight: copying a param with shape torch.Size([7110, 400]) from checkpoint, the shape in current model is torch.Size([4516, 400]).\n\tsize mismatch for models.C15.Decoder.FC_output.bias: copying a param with shape torch.Size([7110]) from checkpoint, the shape in current model is torch.Size([4516]).\n\tsize mismatch for models.C16.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 44666]) from checkpoint, the shape in current model is torch.Size([400, 11486]).\n\tsize mismatch for models.C16.Decoder.FC_output.weight: copying a param with shape torch.Size([44666, 400]) from checkpoint, the shape in current model is torch.Size([11486, 400]).\n\tsize mismatch for models.C16.Decoder.FC_output.bias: copying a param with shape torch.Size([44666]) from checkpoint, the shape in current model is torch.Size([11486]).\n\tsize mismatch for models.C18.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 3254]) from checkpoint, the shape in current model is torch.Size([400, 2169]).\n\tsize mismatch for models.C18.Decoder.FC_output.weight: copying a param with shape torch.Size([3254, 400]) from checkpoint, the shape in current model is torch.Size([2169, 400]).\n\tsize mismatch for models.C18.Decoder.FC_output.bias: copying a param with shape torch.Size([3254]) from checkpoint, the shape in current model is torch.Size([2169]).\n\tsize mismatch for models.C19.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 1623]) from checkpoint, the shape in current model is torch.Size([400, 1072]).\n\tsize mismatch for models.C19.Decoder.FC_output.weight: copying a param with shape torch.Size([1623, 400]) from checkpoint, the shape in current model is torch.Size([1072, 400]).\n\tsize mismatch for models.C19.Decoder.FC_output.bias: copying a param with shape torch.Size([1623]) from checkpoint, the shape in current model is torch.Size([1072]).\n\tsize mismatch for models.C21.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 44245]) from checkpoint, the shape in current model is torch.Size([400, 11655]).\n\tsize mismatch for models.C21.Decoder.FC_output.weight: copying a param with shape torch.Size([44245, 400]) from checkpoint, the shape in current model is torch.Size([11655, 400]).\n\tsize mismatch for models.C21.Decoder.FC_output.bias: copying a param with shape torch.Size([44245]) from checkpoint, the shape in current model is torch.Size([11655]).\n\tsize mismatch for models.C22.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 13]) from checkpoint, the shape in current model is torch.Size([400, 11]).\n\tsize mismatch for models.C22.Decoder.FC_output.weight: copying a param with shape torch.Size([13, 400]) from checkpoint, the shape in current model is torch.Size([11, 400]).\n\tsize mismatch for models.C22.Decoder.FC_output.bias: copying a param with shape torch.Size([13]) from checkpoint, the shape in current model is torch.Size([11]).\n\tsize mismatch for models.C23.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 15]) from checkpoint, the shape in current model is torch.Size([400, 14]).\n\tsize mismatch for models.C23.Decoder.FC_output.weight: copying a param with shape torch.Size([15, 400]) from checkpoint, the shape in current model is torch.Size([14, 400]).\n\tsize mismatch for models.C23.Decoder.FC_output.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([14]).\n\tsize mismatch for models.C24.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 21462]) from checkpoint, the shape in current model is torch.Size([400, 6650]).\n\tsize mismatch for models.C24.Decoder.FC_output.weight: copying a param with shape torch.Size([21462, 400]) from checkpoint, the shape in current model is torch.Size([6650, 400]).\n\tsize mismatch for models.C24.Decoder.FC_output.bias: copying a param with shape torch.Size([21462]) from checkpoint, the shape in current model is torch.Size([6650]).\n\tsize mismatch for models.C25.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 61]) from checkpoint, the shape in current model is torch.Size([400, 52]).\n\tsize mismatch for models.C25.Decoder.FC_output.weight: copying a param with shape torch.Size([61, 400]) from checkpoint, the shape in current model is torch.Size([52, 400]).\n\tsize mismatch for models.C25.Decoder.FC_output.bias: copying a param with shape torch.Size([61]) from checkpoint, the shape in current model is torch.Size([52]).\n\tsize mismatch for models.C26.Encoder.FC_input.weight: copying a param with shape torch.Size([400, 16941]) from checkpoint, the shape in current model is torch.Size([400, 5218]).\n\tsize mismatch for models.C26.Decoder.FC_output.weight: copying a param with shape torch.Size([16941, 400]) from checkpoint, the shape in current model is torch.Size([5218, 400]).\n\tsize mismatch for models.C26.Decoder.FC_output.bias: copying a param with shape torch.Size([16941]) from checkpoint, the shape in current model is torch.Size([5218])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "\n",
    "dataset_path = './criteo/train.csv'\n",
    "\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim  = 784\n",
    "hidden_dim = 400\n",
    "\n",
    "\n",
    "lr = 1e-5\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_keep = [col for col in df.columns if col.startswith('C')]\n",
    "columns_to_keep.append(\"label\")\n",
    "filtered_df = df[columns_to_keep]\n",
    "\n",
    "# Print the shape of the filtered DataFrame\n",
    "print(filtered_df.shape)\n",
    "# Print the columns of the filtered DataFrame\n",
    "print(filtered_df.columns)\n",
    "# Print the number of unique values in each column of the filtered DataFrame\n",
    "num_classes = filtered_df.nunique().sort_values().to_dict()\n",
    "print(num_classes)\n",
    "print(filtered_df.max())\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    A simple implementation of Gaussian MLP Encoder and Decoder\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.LayerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, latent_dim*2)\n",
    "        self.LayerNorm2 = nn.LayerNorm( latent_dim*2)\n",
    "        self.FC_mean = nn.Linear( latent_dim*2, latent_dim)\n",
    "        self.FC_var = nn.Linear( latent_dim*2, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.LayerNorm1(self.FC_input(x)))\n",
    "        h = self.LeakyReLU(self.LayerNorm2(self.FC_input2(h)))\n",
    "\n",
    "        \n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.LayerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.LayerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.LayerNorm1(self.FC_hidden(x)))\n",
    "        h = self.LeakyReLU(self.LayerNorm2(self.FC_hidden2(h)))\n",
    "        x_hat = torch.nn.functional.log_softmax(self.FC_output(h), dim=1)\n",
    "        return x_hat\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.Encoder(x)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x_hat = self.Decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var\n",
    "\n",
    "class ModelsMap(nn.Module):\n",
    "    def __init__(self, columns_to_keep, num_classes, hidden_dim, latent_dim, device):\n",
    "        super(ModelsMap, self).__init__()\n",
    "        self.models = nn.ModuleDict()\n",
    "        self.num_classes = []\n",
    "        self.latent_num_classes = []\n",
    "        self.activation = nn.ReLU()\n",
    "        for col in columns_to_keep:\n",
    "            self.num_classes.append(num_classes[col])\n",
    "            print(f\"{col} has {num_classes[col]} classes\")\n",
    "            self.latent_num_classes.append(latent_dim)\n",
    "            encoder = Encoder(input_dim=num_classes[col], hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "            decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=num_classes[col])\n",
    "            model = Model(Encoder=encoder, Decoder=decoder).to(device)\n",
    "            self.models[col] = model\n",
    "        self.latent_size = len(columns_to_keep) * latent_dim\n",
    "        self.mean_projector = nn.Linear(self.latent_size*2, self.latent_size*3).to(device)\n",
    "\n",
    "        self.mean_projector_1 = nn.Linear(self.latent_size*3, self.latent_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hs = []\n",
    "        for k, v in x.items():\n",
    "            h = self.models[k].encode(v)\n",
    "            hs.append(h)\n",
    "        hs_ = self.mean_projector(torch.cat(hs, dim=1))\n",
    "        hs_ = self.activation(hs_)\n",
    "        hs_=self.mean_projector_1(hs_)\n",
    "        hs_ = torch.split(hs_, self.latent_num_classes, dim=1)\n",
    "        output = {}\n",
    "        for idx, (k, v) in enumerate(x.items()):\n",
    "            mean = self.models[k].Encoder.FC_mean(hs[idx])\n",
    "            # var = self.models[k].Encoder.FC_var(hs[idx])\n",
    "            # z = self.models[k].reparameterization(mean, torch.exp(0.5 * var))\n",
    "            z=mean\n",
    "            x_hat = self.models[k].decode(z)\n",
    "            output[k] = [x_hat, 0, 0]\n",
    "        return output\n",
    "    def encode(self,x):\n",
    "        hs = []\n",
    "        for k, v in x.items():\n",
    "            h = self.models[k].encode(v)\n",
    "            hs.append(h)\n",
    "        hs_ = self.mean_projector(torch.cat(hs, dim=1))\n",
    "        hs_ = self.activation(hs_)\n",
    "        hs_=self.mean_projector_1(hs_)\n",
    "        hs_ = torch.split(hs_, self.latent_num_classes, dim=1)\n",
    "        output = {}\n",
    "        for idx, (k, v) in enumerate(x.items()):\n",
    "            mean = self.models[k].Encoder.FC_mean(hs[idx])\n",
    "            var = self.models[k].Encoder.FC_var(hs[idx])\n",
    "            z = self.models[k].reparameterization(mean, torch.exp(0.5 * var))\n",
    "            output[k] = [z, mean, var]\n",
    "        return output\n",
    "columns_to_keep = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "       'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21',\n",
    "       'C22', 'C23', 'C24', 'C25', 'C26',\"label\"]\n",
    "model = ModelsMap(columns_to_keep, num_classes, hidden_dim, 4, DEVICE)         \n",
    "model.load_state_dict(torch.load('nov_model_weights.pth'))\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the criteox_1 dataset\n",
    "dataset = load_dataset('./criteo')\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "# create a dict with default values as {}\n",
    "from collections import defaultdict\n",
    "c_dict = defaultdict(dict)\n",
    "for col in columns_to_keep:\n",
    "    for i,v in enumerate(set(df[col])):\n",
    "        c_dict[col][v] = i\n",
    "\n",
    "# print(c_dict)\n",
    "import json\n",
    "with open('c_dict.json', 'w') as fp:\n",
    "    json.dump(c_dict, fp)\n",
    "print([num_classes[col] for col in columns_to_keep])\n",
    "    \n",
    "C5_test_dataset = test_dataset.map(lambda x:{col+\"_I\":c_dict[col][x[col]] for col in columns_to_keep},num_proc=32).select_columns([col+\"_I\" for col in columns_to_keep])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total = len(C5_test_dataset)\n",
    "from torch.utils.data import DataLoader\n",
    "model.eval()\n",
    "batch_size=128\n",
    "acc = defaultdict(int)\n",
    "for batch_idx, x in enumerate(DataLoader(C5_test_dataset, batch_size=batch_size)):\n",
    "    xs = {}\n",
    "    x_onehots = {}\n",
    "    for col in columns_to_keep:\n",
    "        x_=x[col+\"_I\"].to(DEVICE)\n",
    "        xs[col]=( x_)\n",
    "        x_onehot = torch.nn.functional.one_hot(x_,num_classes[col]).float()\n",
    "        x_onehots[col]=x_onehot\n",
    "    output = model(x_onehots)\n",
    "    for k,v in output.items():\n",
    "        x_hat, mean, log_var = v\n",
    "        x = xs[k]\n",
    "\n",
    "    ys={}\n",
    "    for k,v in output.items():\n",
    "        x_hat, mean, log_var = v\n",
    "        y = torch.argmax(x_hat,dim=1)\n",
    "        x = xs[k]\n",
    "        ys[k]=(y==x).sum().item()\n",
    "        acc[k]+=ys[k]\n",
    "\n",
    "for k in acc:\n",
    "    print(f\"Accuracy for {k} is {acc[k]/total}\")\n",
    "\n",
    "\n",
    "print(\"Finish!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
